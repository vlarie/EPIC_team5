{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirmed needed dependencies\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "# Dependencies for mapping\n",
    "import gmplot\n",
    "\n",
    "# Dependency for Heat Mapper\n",
    "import gmaps\n",
    "\n",
    "\n",
    "from config import google_API_Key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File inputs/outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "randLatLon_csv = \"./Data/Archived/randomLatLon.csv\" \n",
    "addressList_csv = \"./Data/Archived/addressList.csv\"\n",
    "masterDataCLEAN_csv = \"./Data/masterDataCLEAN.csv\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpful Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################\n",
    "##### VALERIE'S BLOCKS #####\n",
    "###########################\n",
    "\n",
    "# Funtion for reading CSV in as DataFrame\n",
    "def csvDF(oldCSVfilepath):\n",
    "    csvIN = pd.read_csv(oldCSVfilepath)\n",
    "    DF = pd.DataFrame(csvIN)\n",
    "    return DF\n",
    "\n",
    "# Function for converting DataFrame to CSV\n",
    "def DFcsv(dataframe, newCSVfilepath):\n",
    "    dataframe.to_csv(newCSVfilepath, index=False, header=True)\n",
    "    print(f\"Successfully written to '{newCSVfilepath}'\")\n",
    "    \n",
    "# Function for reading in csv, checking for headers, and appending if appropriate\n",
    "def csvDFappend(oldCSVfilepath, newColumn):\n",
    "    csvIN = pd.read_csv(oldCSVfilepath)\n",
    "    DF = pd.DataFrame(csvIN)\n",
    "    # Checking to ensure new header name does not match any current headers\n",
    "    colNames = DF.columns\n",
    "    for value in colNames:\n",
    "        if value == newColumn:\n",
    "            print(\"Cannot append column that matches an existing column name\")\n",
    "            return DF\n",
    "    # Check to ensure length of newColumn matches length of current dataframe columns\n",
    "    if len(newColumn) != len(DF):\n",
    "        print(\"Cannot append column that is not the same length as existing dataframe\")\n",
    "        return DF\n",
    "    # Append newColumn to Dataframe\n",
    "    DF[newColumn] = newColumn\n",
    "    return DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heat Mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Troy's section\n",
    "\n",
    "\n",
    "gmaps.configure(api_key=google_API_Key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell creates a test masterData_df by pulling in Yuta's address file and adds a column as a testm \"value to map\"\n",
    "# This cell can be deleted as soon as there is a master data file that includes a property value column or some other value to plot\n",
    "# The last digit of the zipcode is used as a value that will vary by area and a random number between 0 and 1 is added to create variation in the weights\n",
    "\n",
    "masterData_df = pd.read_csv(addressList_csv)\n",
    "zips = masterData_df[\"zipcode\"]\n",
    "valueToMap = []\n",
    "\n",
    "for zip in zips:\n",
    "    lastDigit = zip[-1:]\n",
    "#    print(last2Digits)\n",
    "    valueToMap.append(int(lastDigit) + random.uniform(0.0,1.0))\n",
    "    \n",
    "masterData_df[\"valueToMap\"] = valueToMap\n",
    "masterData_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell uses gmaps library to create a google heat map from the data in a master data file.\n",
    "# The masterData csv file is taken as input\n",
    "# The lat and lon columns are taken as the coordinates for hte heatmap \n",
    "# The user specified column is taken as the weighting valies fo each coordinate point\n",
    "\n",
    "df = masterData_df\n",
    "columnToMap = 'valueToMap'\n",
    "max_intensity = df[columnToMap].max()\n",
    "\n",
    "fig = gmaps.figure()\n",
    "heatmap_layer = gmaps.heatmap_layer(df[['lat', 'lon']], weights=df[columnToMap], max_intensity=max_intensity, point_radius=10.0)\n",
    "fig.add_layer(heatmap_layer)\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a function version of the cell above\n",
    "# the function takes columnToMap as the weights for the points defined by 'lat' and 'lon' columns in the dataframe\n",
    "# the dataframe can be included as a parameter, if it is not included masterData_df is assumed\n",
    "\n",
    "def heatMapper(columnToMap, df = masterData_df):\n",
    "    \n",
    "    max_intensity = df[columnToMap].max()\n",
    "    \n",
    "    fig = gmaps.figure()\n",
    "    heatmap_layer = gmaps.heatmap_layer(df[['lat', 'lon']], weights=df[columnToMap], max_intensity=max_intensity, point_radius=10.0)\n",
    "    fig.add_layer(heatmap_layer)\n",
    "\n",
    "    return;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatMapper(columnToMap = 'valueToMap')\n",
    "fig"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonData",
   "language": "python",
   "name": "pythondata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
