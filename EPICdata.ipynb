{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirmed needed dependencies\n",
    "import pprint\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# Dependencies for geocoordinates generator\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import gmplot\n",
    "\n",
    "# Dependencies for conversion of coordinates to addresses\n",
    "import geopy\n",
    "from geopy.geocoders import Nominatim\n",
    "import time\n",
    "\n",
    "# Dependencies for Zillow data\n",
    "from pyzillow.pyzillow import ZillowWrapper, GetDeepSearchResults\n",
    "\n",
    "# Dependency for Heat Mapper\n",
    "import gmaps\n",
    "\n",
    "\n",
    "# Add config.py file with the following variables and cooresponding Zillow API keys\n",
    "# from config import Zapi, Zapi01, Zapi02, Zapi03, Zapi04, Zapi05, Zapi06, Zapi07, Zapi08, Zapi09, Zapi10, Zapi11, Zapi12, Zapi13, Zapi14, Zapi15, Zapi16, Zapi17, Zapi18, Zapi19, Zapi20, Ztroy1, Ztroy2, Ztroy3, Zseth, Zseth2, Zkat, Zval, Zyuta\n",
    "from config import google_API_Key\n",
    "\n",
    "\n",
    "################# ONGOING EDITS TO REQUIREMENTS.MD #################\n",
    "###### IF ANY ERRORS OCCUR RELATING TO MODULES OR CONFIG.PY #######\n",
    "### REFER TO requirements.md TO ENSURE YOU ARE PROPERLY SETUP ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File inputs/outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "masterDataCLEAN_csv = \"./Data/masterDataCLEAN.csv\"\n",
    "\n",
    "\n",
    "randLatLon_csv = \"./Data/randomLatLon.csv\" \n",
    "addressList_csv = \"./Data/addressList.csv\"\n",
    "masterData_csv = \"./Data/masterData.csv\"\n",
    "masterDFIMPORTclean_csv = \"./Data/masterDFIMPORTclean.csv\"\n",
    "masterData100_csv = \"./Data/masterData100.csv\"\n",
    "masterData1000_csv = \"./Data/masterData1000.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpful Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################\n",
    "##### VALERIE'S BLOCKS #####\n",
    "###########################\n",
    "\n",
    "# Funtion for reading CSV in as DataFrame\n",
    "def csvDF(oldCSVfilepath):\n",
    "    csvIN = pd.read_csv(oldCSVfilepath)\n",
    "    DF = pd.DataFrame(csvIN)\n",
    "    return DF\n",
    "\n",
    "# Function for converting DataFrame to CSV\n",
    "def DFcsv(dataframe, newCSVfilepath):\n",
    "    dataframe.to_csv(newCSVfilepath, index=False, header=True)\n",
    "    print(f\"Successfully written to '{newCSVfilepath}'\")\n",
    "    \n",
    "# Function for reading in csv, checking for headers, and appending if appropriate\n",
    "def csvDFappend(oldCSVfilepath, newColumn):\n",
    "    csvIN = pd.read_csv(oldCSVfilepath)\n",
    "    DF = pd.DataFrame(csvIN)\n",
    "    # Checking to ensure new header name does not match any current headers\n",
    "    colNames = DF.columns\n",
    "    for value in colNames:\n",
    "        if value == newColumn:\n",
    "            print(\"Cannot append column that matches an existing column name\")\n",
    "            return DF\n",
    "    # Check to ensure length of newColumn matches length of current dataframe columns\n",
    "    if len(newColumn) != len(DF):\n",
    "        print(\"Cannot append column that is not the same length as existing dataframe\")\n",
    "        return DF\n",
    "    # Append newColumn to Dataframe\n",
    "    DF[newColumn] = newColumn\n",
    "    return DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geocoordinates of Austin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################\n",
    "# this section written by troy bailey.   #\n",
    "# enter uservariables below to determine #\n",
    "# center location, radius of circle, and #\n",
    "# number of geocoordinates to generate.  #\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "#### USER VARIABLES ####\n",
    "########################\n",
    "\n",
    "x0 = 30.27444       #### Set center coordiantes in decimal degrees\n",
    "y0 = -97.74028      #### initial coordiantes are location of Texas State Capitol Building\n",
    "\n",
    "radius = 20         #### Set radius in miles\n",
    "\n",
    "points = 40000        #### Set number of lat,lon points to generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables and inputs for coordinate calculations\n",
    "lat_lon_list = []\n",
    "radiusInDegrees=radius/69           \n",
    "r = radiusInDegrees\n",
    "points += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate each coordiante point and build a list of lat and lon\n",
    "for i in range(1, points):\n",
    "    u = float(random.uniform(0.0,1.0)) #random number for radius length\n",
    "    v = float(random.uniform(0.0,1.0)) #random number for pi radians\n",
    "    \n",
    "    w = r * math.sqrt(u) #radius length\n",
    "    t = 2 * math.pi * v  #radians\n",
    "    x = w * math.cos(t)  #calculate x coord distance\n",
    "    y = w * math.sin(t)  #calculate y coord distance\n",
    "    \n",
    "    xLat  = x + x0       #offset x by center x\n",
    "    yLon = y + y0        #offset y by center y\n",
    "    \n",
    "    lat_lon_list.append([xLat,yLon])\n",
    "\n",
    "# convert list to dataframe\n",
    "lat_lon_df = pd.DataFrame(lat_lon_list, columns=['lat','lon'])\n",
    "\n",
    "lat_lon_df.head()\n",
    "\n",
    "len(lat_lon_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a CSV file of coordinate points\n",
    "lat_lon_df.to_csv(randLatLon_csv, index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot coordinate points on map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This cell by Troy\n",
    "# This section will plot points on a Google map centered at centerPointLat and centerPointLon with a magnification of magFactor\n",
    "# It assumes there is a dataframe with \"lat\" and \"lon\" columns\n",
    "# The resulting map is saved to a file called \n",
    "\n",
    "lat_lon_df = pd.read_csv(\"./Data/Archived/randomLatLon.csv\")\n",
    "centerPointLat = 30.27444  #these are the coordinates of the Texas State Capitol building\n",
    "centerPointLon = -97.74028 #these are the coordinates of the Texas State Capitol building\n",
    "magnificationFactor = 10\n",
    "pointColor = \"red\"\n",
    "pointSize = 100\n",
    "mapOutputFile = \"randLatLonMap.html\"\n",
    "df = lat_lon_df\n",
    "\n",
    "gmap = gmplot.GoogleMapPlotter(centerPointLat, centerPointLon, magnificationFactor)\n",
    "\n",
    "gmap.scatter(df[\"lat\"], df[\"lon\"], pointColor, size=pointSize, marker=False)\n",
    "\n",
    "gmap.draw(\"./Presentation/\" + mapOutputFile)\n",
    "\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Coordinates to Residential Addresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "##### Yuta's Blocks #####\n",
    "#########################\n",
    "\n",
    "##### Geopy Nominatim API #####\n",
    "geopy.geocoders.options.default_user_agent = \"ut-group-EPIC\"\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "url = \"https://nominatim.openstreetmap.org/reverse?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test API - Known Residential Address\n",
    "params_1 = {\n",
    "    \"format\": \"jsonv2\",\n",
    "    \"lat\": 30.440777,\n",
    "    \"lon\": -97.777048\n",
    "}\n",
    "\n",
    "print(\"===== Test Home Response:\")\n",
    "response = requests.get(url, params=params_1).json()\n",
    "pp.pprint(response)\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import CSV, put into DataFrame\n",
    "latlon_df = pd.read_csv(randLatLon_csv)\n",
    "latlon_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put latitudes and longitudes into a zip object\n",
    "lats = latlon_df.iloc[:, 0]\n",
    "lons = latlon_df.iloc[:, 1]\n",
    "lat_lons = []\n",
    "lat_lons = zip(lats, lons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Loop Request API for Addresses / Append to lists #####\n",
    "# Make sure to import time\n",
    "\n",
    "query_url = \"https://nominatim.openstreetmap.org/reverse?\"\n",
    "\n",
    "house_num = []\n",
    "road = []\n",
    "postcode = []\n",
    "lat = []\n",
    "lon = []\n",
    "neighborhood = []\n",
    "\n",
    "counter = 1\n",
    "numRequests = latlon_df[\"lat\"].count()\n",
    "rSuccess = []\n",
    "rFailure = []\n",
    "\n",
    "print(f\"Processing {numRequests} Requests...\")\n",
    "\n",
    "# Nominatim API Request\n",
    "\n",
    "for lat_lon in lat_lons:\n",
    "    params = {\n",
    "        \"format\": \"jsonv2\",\n",
    "        \"lat\": lat_lon[0],\n",
    "        \"lon\": lat_lon[1]\n",
    "    }\n",
    "\n",
    "    time.sleep(1.1)\n",
    "    response = requests.get(query_url, params=params).json()\n",
    "\n",
    "    if response['type'] == 'house' or response['type'] == 'yes':\n",
    "        lat.append(response['lat'])\n",
    "        lon.append(response['lon'])\n",
    "        \n",
    "        try:\n",
    "            postcode.append(response['address']['postcode'])\n",
    "        except (KeyError, IndexError):\n",
    "            postcode.append(\"NA\")\n",
    "        try:\n",
    "            house_num.append(response['address']['house_number'])\n",
    "        except (KeyError, IndexError):\n",
    "            house_num.append(\"NA\")\n",
    "        try:\n",
    "            road.append(response['address']['road'])\n",
    "        except (KeyError, IndexError):\n",
    "            road.append(\"NA\")\n",
    "        try:\n",
    "            neighborhood.append(response['address']['neighbourhood'])\n",
    "        except (KeyError, IndexError):\n",
    "            neighborhood.append(\"NA\")\n",
    "        \n",
    "        print(f\"Processed Record {counter} of {numRequests}.\")\n",
    "        rSuccess.append(counter)\n",
    "        counter += 1\n",
    "        \n",
    "    else:\n",
    "        print(f\"Wrong Type - Skipped Record {counter} of {numRequests}.\")\n",
    "        rFailure.append(counter)\n",
    "        counter += 1\n",
    "        \n",
    "print(f\"Finished Requests !!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Request Results:\")\n",
    "print(\"Success #:\" + str(len(rSuccess)))\n",
    "print(\"Skipped #:\" + str(len(rFailure)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe with addresses from API requests\n",
    "address_df = pd.DataFrame({\n",
    "    \"house #\": house_num,\n",
    "    \"street\": road,\n",
    "    \"zipcode\": postcode,\n",
    "    \"lat\": lat,\n",
    "    \"lon\": lon,\n",
    "    \"neighborhood\": neighborhood,\n",
    "})\n",
    "\n",
    "# Clean up Dataframe Columns before output (Drop incomplete zipcodes, Highway streets, and Null house # or streets)\n",
    "address_df = address_df[address_df['zipcode'].str.len() == 5]\n",
    "address_df = address_df[address_df['zipcode'].apply(lambda x: len(str(x)) > 3)]\n",
    "address_df = address_df[address_df['street'].str.contains(\"Highway\") == False]\n",
    "address_df = address_df[address_df['house #'].str.contains(\"NA\") == False]\n",
    "address_df = address_df[address_df['street'].str.contains(\"NA\") == False]\n",
    "address_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a CSV file of addresses\n",
    "address_df.to_csv(addressList_csv, index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map out CSV with gmplot\n",
    "\n",
    "addressList_csv = \"./Data/addressList.csv\"\n",
    "\n",
    "gmap = gmplot.GoogleMapPlotter(30.27444, -97.74028, 10)\n",
    "\n",
    "gmap.scatter(addressList_csv[\"lat\"], addressList_csv[\"lon\"], 'red', size=20, marker=False)\n",
    "\n",
    "gmap.draw(\"./Visuals/myaddressmap.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Addresses on a Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20177"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This cell by Troy\n",
    "# This section will plot points on a Google map centered at centerPointLat and centerPointLon with a magnification of magFactor\n",
    "# It plots the addresses we have selected from the random Lat Lon points\n",
    "# The resulting map is saved to a file called addressMap.html\n",
    "\n",
    "address_df = pd.read_csv(\"./Data/Archived/addressList.csv\")\n",
    "centerPointLat = 30.27444  #these are the coordinates of the Texas State Capitol building\n",
    "centerPointLon = -97.74028 #these are the coordinates of the Texas State Capitol building\n",
    "magnificationFactor = 10\n",
    "pointColor = \"blue\"\n",
    "pointSize = 100\n",
    "mapOutputFile = \"addressMap.html\"\n",
    "df = address_df\n",
    "\n",
    "gmap = gmplot.GoogleMapPlotter(centerPointLat, centerPointLon, magnificationFactor)\n",
    "\n",
    "gmap.scatter(df[\"lat\"], df[\"lon\"], pointColor, size=pointSize, marker=False)\n",
    "\n",
    "gmap.draw(\"./Presentation/\" + mapOutputFile)\n",
    "\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zillow API Calls using Address and Zipcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################\n",
    "##### VALERIE'S BLOCKS #####\n",
    "###########################\n",
    "\n",
    "# Function for reading in csv, checking for headers, and appending if appropriate\n",
    "def csvDFappend(oldCSVfilepath, newColumn):\n",
    "    csvIN = pd.read_csv(oldCSVfilepath)\n",
    "    DF = pd.DataFrame(csvIN)\n",
    "    # Checking to ensure new header name does not match any current headers\n",
    "    colNames = DF.columns\n",
    "    for value in colNames:\n",
    "        if value == newColumn:\n",
    "            print(\"Cannot append column that matches an existing column name\")\n",
    "            return DF\n",
    "    # Check to ensure length of newColumn matches length of current dataframe columns\n",
    "    if len(newColumn) != len(DF):\n",
    "        print(\"Cannot append column that is not the same length as existing dataframe\")\n",
    "        return DF\n",
    "    # Append newColumn to Dataframe\n",
    "    DF[newColumn] = newColumn\n",
    "    return DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEST BLCOK FOR FUNCTION ELEMENTS OF csvDFappend ###\n",
    "\n",
    "colNames = masterDFIMPORTclean.columns\n",
    "newCol = \"zipcode\"\n",
    "for value in colNames:\n",
    "    print(value)\n",
    "    if value == newCol:\n",
    "        print(\"Cannot append column that matches an existing column name\")\n",
    "        break\n",
    "        \n",
    "if len(address_df[\"zipcode\"]) != len(masterDFIMPORTclean):\n",
    "    print(\"Cannot append column that is not the same length as existing dataframe\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Tiny sample to work with looping without exhausting API call limits\n",
    "addressListTiny_csv = \"./Data/addressListTiny.csv\"\n",
    "\n",
    "address_sample = pd.read_csv(addressListTiny_csv)\n",
    "address_df = pd.DataFrame(address_sample)\n",
    "print(len(address_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Funtion for reading CSV in as DataFrame\n",
    "def csvDF(oldCSVfilepath):\n",
    "    csvIN = pd.read_csv(oldCSVfilepath)\n",
    "    DF = pd.DataFrame(csvIN)\n",
    "    return DF\n",
    "\n",
    "# Function for converting DataFrame to CSV\n",
    "def DFcsv(dataframe, newCSVfilepath):\n",
    "    newCSVfilepath = pd.to_csv(dataframe)\n",
    "    print(f\"{dataframe} successfully written to {newCSVfilepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "addressDF = csvDF(addressList_csv)\n",
    "print(len(addressDF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "############### LOOPING FUNCTION FULLY OPERATIONAL ################\n",
    "####### HOWEVER, ZILLOW ONLY ALLOWS 1000 API CALLS PER DAY #######\n",
    "\n",
    "# Zillow API call function\n",
    "def zCall(API, index, address, zipcode):\n",
    "    APIkey = API[index]\n",
    "    zillow_data = ZillowWrapper(APIkey)\n",
    "    deep_search_response = zillow_data.get_deep_search_results(address, zipcode)\n",
    "    result = GetDeepSearchResults(deep_search_response)\n",
    "    return result\n",
    "\n",
    "# List containers for collected property data\n",
    "zid = []\n",
    "addresses = []\n",
    "alats = []\n",
    "alons = []\n",
    "valuation = []\n",
    "sqft = []\n",
    "\n",
    "# List of Zillow API keys to loop through due to daily API call limits\n",
    "zAPIs = [Zapi, Zapi01, Zapi02, Zapi03, Zapi04, Zapi05, Zapi06, Zapi07, Zapi08, Zapi09, \n",
    "         Zapi10, Zapi11, Zapi12, Zapi13, Zapi14, Zapi15, Zapi16, Zapi17, Zapi18, Zapi19, \n",
    "         Zapi20, Ztroy1, Ztroy2, Ztroy3, Zseth, Zseth2, Zkat, Zval, Zyuta]\n",
    "index = 0\n",
    "    \n",
    "for row, home in addressDF.iterrows():\n",
    "    address = str(addressDF[\"house #\"][row]) + \" \" + str(addressDF[\"street\"][row])\n",
    "    addresses.append(address)\n",
    "    zipcode = addressDF[\"zipcode\"][row]\n",
    "    print(f\"Processing {address}, {zipcode} (index {row}).\")\n",
    "    \n",
    "    result = None\n",
    "    try:\n",
    "        try:\n",
    "            result = zCall(zAPIs, index, address, zipcode)\n",
    "            print(f\"{row} Success!\")\n",
    "        except KeyError:  ### ERROR FOR API CALL LIMIT EXCEEDED ###\n",
    "            print(f\"KeyError has occurred for {address}, {zipcode} (index {row}).\")\n",
    "            index += 1\n",
    "            print(f\"Proceeding to API[{index}]\")\n",
    "            if index >= len(zAPIs):\n",
    "                print(f\"API[{index}] does not exist. Need more API keys to complete analysis.\")\n",
    "                break\n",
    "            result = zCall(zAPIs, index, address, zipcode)\n",
    "\n",
    "    except:\n",
    "        print(f\"No record found for {address}, {zipcode} (index {row}). Appending lists with null values\")\n",
    "        zid.append(None)\n",
    "        alats.append(None)\n",
    "        alons.append(None)\n",
    "        valuation.append(None)\n",
    "        sqft.append(None)\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        zillowID = result.zillow_id\n",
    "        zid.append(zillowID)\n",
    "    except:\n",
    "        print(f\"No zid found for {address}, {zipcode} (index {row}). Appending list with null values\")\n",
    "        zid.append(None)\n",
    "\n",
    "    try:\n",
    "        alat = result.latitude\n",
    "        alats.append(alat)\n",
    "    except:\n",
    "        print(f\"No alat found for {address}, {zipcode} (index {row}). Appending list with null values\")\n",
    "        alats.append(None)\n",
    "\n",
    "    try:\n",
    "        alon = result.longitude\n",
    "        alons.append(alon)\n",
    "    except:\n",
    "        print(f\"No alon found for {address}, {zipcode} (index {row}). Appending list with null values\")\n",
    "        alons.append(None)\n",
    "\n",
    "    try:    \n",
    "        val = int(result.zestimate_amount)\n",
    "        valuation.append(val)\n",
    "    except:\n",
    "        print(f\"No valuation found for {address}, {zipcode} (index {row}). Appending list with null values\")\n",
    "        valuation.append(None)\n",
    "\n",
    "    try:\n",
    "        zsqft = int(result.home_size)\n",
    "        sqft.append(zsqft)\n",
    "    except:\n",
    "        print(f\"No sqft found for {address}, {zipcode} (index {row}). Appending list with null values\")\n",
    "        sqft.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(addresses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Master Dataframe Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking to ensure lists are appropriate lengths\n",
    "print(len(zid))\n",
    "print(len(alats))\n",
    "print(len(alons))\n",
    "print(len(addresses))\n",
    "print(len(valuation))\n",
    "print(len(sqft))\n",
    "print(len(valsqft))\n",
    "\n",
    "# Referring back to addressList_csv generated dataframe for relevant info\n",
    "addressDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FULL SAMPLE FROM 40,000 RANDOM LAT, LON GENERATION\n",
    "\n",
    "masterDF = pd.DataFrame({\n",
    "    \"Zillow ID\": zid,\n",
    "    \"address\": addresses,\n",
    "    \"zipcode\": addressDF[\"zipcode\"],\n",
    "    \"alat\": alats,\n",
    "    \"alon\": alons,\n",
    "    \"valuation\": valuation,\n",
    "    \"sqft\": sqft,\n",
    "#     \"value sqft\": valsqft,\n",
    "    \"neighborhood\": addressDF[\"neighborhood\"],\n",
    "})\n",
    "print(len(masterDF))\n",
    "masterDF.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masterDFclean = masterDF.dropna(how=\"any\", subset=[\"Zillow ID\"])\n",
    "len(masterDFclean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# masterDF to csv\n",
    "masterDFclean.to_csv(masterData_csv, index=False, header=True)\n",
    "masterDFclean.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zillow Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masterDF = csvDF(masterData_csv)\n",
    "print(len(masterDF))\n",
    "masterDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA CLEANING #\n",
    "# Dropping duplicates\n",
    "masterDFdrops = masterDF.drop_duplicates(subset=[\"Zillow ID\"], keep=\"first\")\n",
    "print(len(masterDFdrops))\n",
    "masterDFdrops.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning out None values for \"valuation\"\n",
    "masterDFdrops = masterDFdrops.dropna(how=\"any\", subset=[\"valuation\"])\n",
    "print(len(masterDFdrops))\n",
    "masterDFdrops.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning out None values for \"sqft\" \n",
    "masterDFdrops = masterDFdrops.dropna(how=\"any\", subset=[\"sqft\"])\n",
    "print(len(masterDFdrops))\n",
    "masterDFdrops.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Value per Sqft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate \"value sqft\" after None value rows removed for \"valuation\" and \"sqft\"\n",
    "\n",
    "valsqft = []\n",
    "for row, value in masterDFdrops.iterrows():\n",
    "    try:\n",
    "        vsqft = round((masterDFdrops[\"valuation\"][row] / masterDFdrops[\"sqft\"][row]), 2)\n",
    "        valsqft.append(vsqft)\n",
    "    except: ### THIS ERROR SHOULD NO LONGER PRINT BECAUSE NONE VALUES WERE PREVIOUSLY REMOVED\n",
    "        print(\"Cannot perform math with NoneType\")\n",
    "        valsqft.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking to ensure lists are appropriate lengths\n",
    "print(len(masterDFdrops))\n",
    "print(len(valsqft))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding \"value sqft\" column\n",
    "masterDFdrops[\"value sqft\"] = valsqft\n",
    "\n",
    "# Reordering columns\n",
    "masterDFdrops = masterDFdrops[['Zillow ID', 'address', 'zipcode', 'alat', 'alon', 'valuation', 'sqft', \n",
    "                               'value sqft', 'neighborhood', 'tractCode', 'countyFips', 'stateFips', 'commuteTime']]\n",
    "masterDFdrops.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DFcsv(masterDFdrops, masterDFIMPORTclean_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masterDFcleaning = csvDF(masterDFIMPORTclean_csv)\n",
    "masterDFcleaning.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for finding and dropping rows with nonsense values (e.g. valuation > $10,000,000)\n",
    "def dropNonsense(dataframe, columnName, minVal, maxVal):\n",
    "    dropIndices = []\n",
    "    for index, row in dataframe.iterrows():\n",
    "        val = dataframe[columnName][index]\n",
    "        if (maxVal is not None) and (val > maxVal) or (minVal is not None) and (val < minVal):\n",
    "            dropIndices.append(index)\n",
    "    return dataframe.drop(index=dropIndices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish reasonable values for columns\n",
    "reasonableVals = [\n",
    "    (\"valuation\", 20000, 10000000),\n",
    "    (\"sqft\", 500, 10000),\n",
    "    (\"value sqft\", 0, 1500)\n",
    "]\n",
    "\n",
    "# Loop through dataframe to drop nonsense data\n",
    "masterDFcleaning = masterDFcleaning\n",
    "for entry in reasonableVals:\n",
    "    masterDFcleaning = dropNonsense(masterDFcleaning, entry[0], entry[1], entry[2])\n",
    "\n",
    "# Confirm count\n",
    "len(masterDFcleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Checking high and low values with sort\n",
    "masterDFcleaning.sort_values(by=\"value sqft\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(masterDFcleaning))\n",
    "DFcsv(masterDFcleaning, masterDataCLEAN_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging Master Data with Commute Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "commuteDF = csvDF(\"./Data/masterData.csv\")\n",
    "commuteDF.head()\n",
    "print(len(commuteDF))\n",
    "print(len(masterDFcleaning))\n",
    "print(\"Commute\")\n",
    "print(commuteDF.dtypes)\n",
    "print(\"Master\")\n",
    "print(masterDFcleaning.dtypes)\n",
    "commuteDF.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MASTERdf = pd.merge(masterDFcleaning, \n",
    "                    commuteDF[['tractCode', 'countyFips', 'stateFips', 'commuteTime']], \n",
    "                    how=\"left\",\n",
    "                    on='Zillow ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(MASTERdf))\n",
    "MASTERdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Zillow Matched Addresses on a Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8530"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This cell by Troy\n",
    "# This section will plot points on a Google map centered at centerPointLat and centerPointLon with a magnification of magFactor\n",
    "# It plots the Zillow addresses we have matched from our randomly selected \n",
    "# The resulting map is saved to a file called addressMap.html\n",
    "\n",
    "masterData_df = pd.read_csv(\"./Data/masterDataCLEAN.csv\")\n",
    "centerPointLat = 30.27444  #these are the coordinates of the Texas State Capitol building\n",
    "centerPointLon = -97.74028 #these are the coordinates of the Texas State Capitol building\n",
    "magnificationFactor = 10\n",
    "pointColor = \"green\"\n",
    "pointSize = 100\n",
    "mapOutputFile = \"masterDataMap.html\"\n",
    "df = masterData_df\n",
    "\n",
    "gmap = gmplot.GoogleMapPlotter(centerPointLat, centerPointLon, magnificationFactor)\n",
    "\n",
    "gmap.scatter(df[\"alat\"], df[\"alon\"], pointColor, size=pointSize, marker=False)\n",
    "\n",
    "gmap.draw(\"./Presentation/\" + mapOutputFile)\n",
    "\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crime Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kat's section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## School Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seth's section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Commute Time Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Troy's section\n",
    "\n",
    "# This section reads the masterData csv file and uses the lat lon coordinates to get the us census tract code for that address\n",
    "# It then uses the tract code to access the average commute times reported for that tract\n",
    "\n",
    "\n",
    "# Load in data frame from file with lat and lon\n",
    "masterData_df = pd.read_csv(masterData_csv)\n",
    "\n",
    "\n",
    "lats = masterData_df['alat']\n",
    "lons = masterData_df['alon']\n",
    "\n",
    "# Set up arrays for new data\n",
    "tractCodeList = []\n",
    "countyFipsList = []\n",
    "stateFipsList = []\n",
    "\n",
    "\n",
    "# Use us census API to get state fips, county fips, and tract code for all addr in dataframe\n",
    "for lat, lon in zip(lats,lons):\n",
    "    print(\"getting data for \" + str(lat) + str(lon))\n",
    "    targetUrl = \"https://geocoding.geo.census.gov/geocoder/geographies/coordinates?x=\" + str(lon) + \"&y=\" + str(lat) + \"&benchmark=Public_AR_Census2010&vintage=Census2010_Census2010&layers=14&format=json\"\n",
    "    results = requests.get(targetUrl).json()\n",
    "    print(results)\n",
    "    tractCodeList.append(results[\"result\"][\"geographies\"][\"Census Blocks\"][0][\"TRACT\"])\n",
    "    countyFipsList.append(results[\"result\"][\"geographies\"][\"Census Blocks\"][0][\"COUNTY\"])\n",
    "    stateFipsList.append(results[\"result\"][\"geographies\"][\"Census Blocks\"][0][\"STATE\"])\n",
    "    \n",
    "# Load new data into masterData\n",
    "masterData_df[\"tractCode\"] = tractCodeList\n",
    "masterData_df[\"countyFips\"] = countyFipsList\n",
    "masterData_df[\"stateFips\"] = stateFipsList\n",
    "\n",
    "masterData_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masterData_df.to_csv(masterData_csv, index=False, header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell uses us census state fips, county fips, and tract code to access commute times\n",
    "\n",
    "from config import census_API_Key\n",
    "\n",
    "# Set up list to hold new data\n",
    "commuteTimeList = []\n",
    "\n",
    "loopCounter = 0\n",
    "\n",
    "# Use us census API to get commute time from state, county and tract\n",
    "for state, county, tract in zip(stateFipsList, countyFipsList ,tractCodeList):\n",
    "    print(\"getting data for \" + state + county + tract)\n",
    "    loopCounter += 1\n",
    "    print(loopCounter)\n",
    "    targetUrl = \"https://api.census.gov/data/2016/acs/acs5/profile?get=DP03_0025E,NAME&for=tract:\" + tract + \"&in=state:\" + state + \" county:\" + county + \"&key=\" + census_API_Key\n",
    "    results = requests.get(targetUrl).json()\n",
    "    #print(results)\n",
    "    commuteTimeList.append(results[1][0])\n",
    "\n",
    "# Add commute time to masterData_df\n",
    "masterData_df[\"commuteTime\"] = commuteTimeList\n",
    "\n",
    "# write a CSV\n",
    "#masterData_df.to_csv(masterData_csv, index=False, header=True)\n",
    "\n",
    "masterData_df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masterData_df.to_csv(masterData_csv, index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heat Mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Troy's section\n",
    "\n",
    "\n",
    "gmaps.configure(api_key=google_API_Key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell creates a test masterData_df by pulling in Yuta's address file and adds a column as a testm \"value to map\"\n",
    "# This cell can be deleted as soon as there is a master data file that includes a property value column or some other value to plot\n",
    "# The last digit of the zipcode is used as a value that will vary by area and a random number between 0 and 1 is added to create variation in the weights\n",
    "\n",
    "masterData_df = pd.read_csv(addressList_csv)\n",
    "zips = masterData_df[\"zipcode\"]\n",
    "valueToMap = []\n",
    "\n",
    "for zip in zips:\n",
    "    lastDigit = zip[-1:]\n",
    "#    print(last2Digits)\n",
    "    valueToMap.append(int(lastDigit) + random.uniform(0.0,1.0))\n",
    "    \n",
    "masterData_df[\"valueToMap\"] = valueToMap\n",
    "masterData_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell uses gmaps library to create a google heat map from the data in a master data file.\n",
    "# The masterData csv file is taken as input\n",
    "# The lat and lon columns are taken as the coordinates for hte heatmap \n",
    "# The user specified column is taken as the weighting valies fo each coordinate point\n",
    "\n",
    "df = masterData_df\n",
    "columnToMap = 'valueToMap'\n",
    "max_intensity = df[columnToMap].max()\n",
    "\n",
    "fig = gmaps.figure()\n",
    "heatmap_layer = gmaps.heatmap_layer(df[['lat', 'lon']], weights=df[columnToMap], max_intensity=max_intensity, point_radius=10.0)\n",
    "fig.add_layer(heatmap_layer)\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a function version of the cell above\n",
    "# the function takes columnToMap as the weights for the points defined by 'lat' and 'lon' columns in the dataframe\n",
    "# the dataframe can be included as a parameter, if it is not included masterData_df is assumed\n",
    "\n",
    "def heatMapper(columnToMap, df = masterData_df):\n",
    "    \n",
    "    max_intensity = df[columnToMap].max()\n",
    "    \n",
    "    fig = gmaps.figure()\n",
    "    heatmap_layer = gmaps.heatmap_layer(df[['lat', 'lon']], weights=df[columnToMap], max_intensity=max_intensity, point_radius=10.0)\n",
    "    fig.add_layer(heatmap_layer)\n",
    "\n",
    "    return;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatMapper(columnToMap = 'valueToMap')\n",
    "fig"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
